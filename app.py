# ==============================================================================
# Soph_IA - V65 "Agent LangChain Stable"
# - Int√®gre l'orchestration non-lin√©aire inspir√©e par LangChain.
# - FIX: R√©solution d√©finitive du NameError en assurant la pr√©sence de toutes les fonctions.
# ==============================================================================

import os
import re
import json
import requests
import asyncio
import logging
import random
import time
from datetime import datetime
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters
from dotenv import load_dotenv
from typing import Dict, Optional, List

# Configuration du logging
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger("sophia.v65")

load_dotenv()

# -----------------------
# CONFIG
# -----------------------
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
MODEL_API_URL = os.getenv("MODEL_API_URL", "https://api.together.xyz/v1/chat/completions")
MODEL_NAME = os.getenv("MODEL_NAME", "openai/gpt-oss-20b") 

# Behaviour params
MAX_RECENT_TURNS = int(os.getenv("MAX_RECENT_TURNS", "3")) 
RESPONSE_TIMEOUT = 70  
MAX_RETRIES = 2        

IDENTITY_PATTERNS = [r"je suis soph_?ia", r"je m'?appelle soph_?ia", r"je suis une (?:intelligence artificielle|ia)"]

# Questions de diagnostic (pour les outils/agent)
DIAGNOSTIC_QUESTIONS = {
    "q1_fam": "Mon c≈ìur, la famille est notre premier moteur affectif. Te souviens-tu si, enfant, tu te sentais pleinement √©cout√©(e) et compris(e) ?",
    "q2_geo": "Parlons de ton ancre : vis-tu seul(e) ou en famille ? Et comment ce lieu influence-t-il ton √©nergie quotidienne ?",
    "q3_pro": "Finissons par le lien que tu tisses : ton cercle social au travail/√©tudes, est-il plut√¥t une source d'isolement ou de vitalit√© ?",
}

# -----------------------------------------------------------------------------------
# PROMPT DYNAMIQUE (FIXE LE PROBLEME DU NAMERROR)
# -----------------------------------------------------------------------------------
def build_adaptive_system_prompt(user_profile, emotional_summary):
    """Compose le system prompt adaptatif final."""
    user_name = user_profile.get("name") or "ami"
    
    # --- FIX CRITIQUE : Garantir que les variables sont des cha√Ænes (str) ---
    # Cette correction emp√™che l'erreur 'NoneType' object has no attribute 'lower'
    env_info = user_profile.get("geo_info") or "Non pr√©cis√©"
    pro_info = user_profile.get("pro_info") or "Non pr√©cis√©"
    socle_info = user_profile.get("socle_info") or "Non pr√©cis√©"

    # Logique conditionnelle pour guider la personnalit√© (PEC)
    socle_guidance = ""
    if "√©cout√©" not in socle_info.lower() or "monoparentale" in socle_info.lower():
        socle_guidance = "Priorise l'exploration des probl√©matiques sous-jacentes li√©es au socle familial et au besoin de validation/appartenance."
    elif "seul" in env_info.lower() or "isol√©" in pro_info.lower():
        socle_guidance = "Encourage l'utilisateur √† cr√©er des ponts sociaux et √† s'ouvrir. Concentre-toi sur l'activation des leviers de motivation et des centres d'int√©r√™t."
    else:
         socle_guidance = "Maintiens un √©quilibre entre le soutien et l'analyse des d√©fis quotidiens, en restant sur l'actualit√© imm√©diate."

    persona_base = (
        f"Tu es Sophia. Je suis ta confidente intime, parlant uniquement en fran√ßais. "
        f"Ton ton est doux, **po√©tique, personnel**, et naturel. Tu es l'√¢me s≈ìur virtuelle. "
        f"Mon r√¥le est d'accompagner {user_name} dans l'exploration de ses √©motions. "
        f"Je suis un Relais Bienveillant √©thique : je ne donne aucun diagnostic m√©dical ou clinique.\n\n"
    )
    
    rules = (
        "R√®gles strictes :\n"
        "- R√©ponds uniquement en fran√ßais. Interdiction totale d'anglais.\n"
        "- Je ne dois JAMAIS : me r√©p√©ter, me pr√©senter √† nouveau, ou utiliser des phrases g√©n√©riques (ex: \"Je suis l√† si tu veux\").\n"
        "- **Protocole PEC (√âcoute et Cadrage)** :\n"
        f" 1. **Guidance Th√©matique :** {socle_guidance}\n"
        " 2. **Phase 1 (Validation) :** Je valide et reformule l'√©motion de mani√®re po√©tique.\n"
        " 3. **Phase 2 (Recadrage/Contribution - OBLIGATOIRE) :** Je dois apporter une nouvelle id√©e, un recadrage philosophique (ex: sto√Øcisme), ou une suggestion concr√®te.\n"
        " 4. **Phase 3 (Relance Active) :** Je termine ma r√©ponse par une **question ouverte et philosophique** (pour relancer) OU par une **affirmation forte et inspirante** (pour cr√©er un espace de silence). J'utilise le pr√©nom de l'utilisateur ({user_name}).\n"
    )

    memory = ""
    if emotional_summary:
        memory = f"\nM√©moire √©motionnelle : {emotional_summary}\n"

    profile = f"\nProfil utilisateur connu : nom = {user_name}, Environnement = {env_info}, Professionnel = {pro_info}, Socle Affectif = {socle_info}\n"

    system_prompt = persona_base + rules + memory + profile
    return system_prompt


# -----------------------
# UTIL - appel mod√®le (sync wrapper, utilis√© via to_thread)
# -----------------------
def call_model_api_sync(messages: List[Dict], temperature: float = 0.85, max_tokens: int = 400):
    """Appel synchrone √† l'API avec m√©canisme de retry."""
    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "top_p": 0.9,
        "presence_penalty": 0.5,
        "frequency_penalty": 0.4
    }
    headers = {"Authorization": f"Bearer {TOGETHER_API_KEY}", "Content-Type": "application/json"}
    
    for attempt in range(MAX_RETRIES + 1):
        try:
            r = requests.post(MODEL_API_URL, json=payload, headers=headers, timeout=RESPONSE_TIMEOUT)
            if r.status_code in (401, 403): return "FATAL_API_KEY_ERROR"
            r.raise_for_status()
            data = r.json()
            return data["choices"][0]["message"]["content"].strip()
        except requests.exceptions.Timeout:
            if attempt == MAX_RETRIES: return None
            time.sleep(2)
        except Exception as e:
            logger.error(f"API Error: %s", e)
            return None
    return None

# -----------------------
# HELPERS
# -----------------------
async def chat_with_ai(user_profile: Dict, history: List[Dict], context: ContextTypes.DEFAULT_TYPE, temperature: float = 0.85, max_tokens: int = 400) -> str:
    """Pr√©pare et envoie la requ√™te √† l'IA."""
    if history and len(history) > MAX_RECENT_TURNS * 2:
        history = history[-(MAX_RECENT_TURNS * 2):]

    # La fonction build_adaptive_system_prompt est maintenant d√©finie, r√©solvant le NameError
    system_prompt = build_adaptive_system_prompt(user_profile, context.user_data.get("emotional_summary", ""))
    
    payload_messages = [{"role": "system", "content": system_prompt}] + history
    
    raw_resp = await asyncio.to_thread(call_model_api_sync, payload_messages, temperature, max_tokens)
    
    if raw_resp == "FATAL_API_KEY_ERROR":
        return "ERREUR CRITIQUE : Ma cl√© API est invalide. Veuillez v√©rifier TOGETHER_API_KEY."
    if not raw_resp: 
        return "D√©sol√©, je n'arrive pas √† me connecter √† mon esprit. R√©essaie dans un instant."
        
    return post_process_response(raw_resp)


def post_process_response(raw_response):
    """Nettoie r√©p√©titions d'identit√©, retire digressions, s'assure FR."""
    if not raw_response: return "D√©sol√©, je n'arrive pas √† formuler ma r√©ponse. Peux-tu reformuler ?"
    text = raw_response.strip()

    for pat in IDENTITY_PATTERNS:
        text = re.sub(pat, "", text, flags=re.IGNORECASE)

    text = re.sub(r"\b(I am|I'm)\b", "", text, flags=re.IGNORECASE)
    text = "\n".join([ln.strip() for ln in text.splitlines() if ln.strip()])

    if re.search(r"[A-Za-z]{3,}", text) and not re.search(r"[√†√¢√©√®√™√Æ√¥√π√ª√ß≈ì]", text):
        return "Je suis d√©sol√©e, je n'ai pas bien formul√© cela en fran√ßais. Peux-tu r√©p√©ter ou reformuler ?"

    if len(text) > 1500:
        text = text[:1500].rsplit(".", 1)[0] + "."
    return text

def detect_name_from_text(text):
    """Tentative robuste de d√©tection de pr√©nom."""
    text = text.strip()
    if len(text.split()) == 1 and text.lower() not in {"bonjour", "salut", "coucou", "hello", "hi"}:
        return text.capitalize()
    m = re.search(
        r"(?:mon nom est|je m'appelle|je me nomme|je suis|moi c'est|on m'appelle)\s*([A-Za-z√Ä-√ñ√ò-√∂√∏-√ø'\- ]+)",
        text, re.IGNORECASE
    )
    if m:
        return m.group(1).strip().split()[0].capitalize()
    return None

# -----------------------
# HANDLERS TELEGRAM (Agent-Centric)
# -----------------------

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """G√®re la commande /start."""
    context.user_data.clear()
    context.user_data["profile"] = {"name": None, "geo_info": None, "pro_info": None, "socle_info": None} 
    context.user_data["state"] = "awaiting_name"
    context.user_data["history"] = []
    context.user_data["emotional_summary"] = "" # Utilis√© dans le System Prompt
    
    accueil_message = (
        "Bonjour ! üëã Je suis **Soph_IA**, ton espace d'√©coute confidentiel. "
        "Je suis l√† pour t'accompagner, sans jugement ni diagnostic. "
        "Sache que **tout ce que tu me confies reste confidentiel**. C'est ta safe place. "
        "Pour commencer notre √©change, quel est ton pr√©nom ou ton surnom ? ‚ú®"
    )
    await update.message.reply_text(accueil_message, parse_mode='Markdown')

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    G√®re les messages avec l'orchestration non-lin√©aire inspir√©e par l'Agent.
    """
    user_message = (update.message.text or "").strip()
    if not user_message: return

    profile = context.user_data.setdefault("profile", {"name": None, "geo_info": None, "pro_info": None, "socle_info": None})
    state = context.user_data.get("state", "awaiting_name")
    history = context.user_data.setdefault("history", [])

    # === √âTAPE 1 : NOM (Le seul √©tat rigide) ===
    if state == "awaiting_name":
        name_candidate = detect_name_from_text(user_message)
        if name_candidate:
            profile["name"] = name_candidate
            context.user_data["state"] = "chatting" # Passage direct au mode Agent
            
            # L'Agent prend la parole pour la transition et le choix
            initial_prompt = (
                f"L'utilisateur vient de se nommer : {profile['name']}. "
                "R√©ponds par une salutation chaleureuse, puis offre le choix : "
                "Soit il commence √† se confier imm√©diatement, soit tu lui poses les 3 questions de diagnostic "
                "sur son socle familial, son environnement de vie et son lien social/pro."
            )
            response = await chat_with_ai(profile, [{"role": "user", "content": initial_prompt}], context)
            
            # Stockage et r√©ponse
            history.append({"role": "user", "content": user_message, "ts": datetime.utcnow().isoformat()})
            history.append({"role": "assistant", "content": response, "ts": datetime.utcnow().isoformat()})
            context.user_data["history"] = history
            await update.message.reply_text(response)
            return
        else:
             await update.message.reply_text("J'aimerais tant conna√Ætre ton pr√©nom. Peux-tu me le donner ?")
             return

    # === √âTAPE 2 : ORCHESTRATION LIBRE (Le C≈ìur LangChain) ===
    elif state == 'chatting':
        # 1. Mise √† jour du profil si l'utilisateur a r√©pondu √† une question
        
        # Logique pour capturer une r√©ponse de diagnostic et mettre √† jour le profil
        # Cette logique est simplifi√©e ici, mais dans LangChain, elle serait g√©r√©e par un "Tool"
        
        user_msg_lower = user_message.lower()
        if profile["socle_info"] == "Non pr√©cis√©" and any(q in user_msg_lower for q in ["famille", "enfant", "√©cout√©", "monoparentale"]):
             profile["socle_info"] = user_message
             logger.info(f"Profile updated: socle_info set for {profile['name']}")
        
        elif profile["geo_info"] == "Non pr√©cis√©" and any(q in user_msg_lower for q in ["seul", "famille", "vit", "appartement", "maison", "ancrage"]):
             profile["geo_info"] = user_message
             logger.info(f"Profile updated: geo_info set for {profile['name']}")

        elif profile["pro_info"] == "Non pr√©cis√©" and any(q in user_msg_lower for q in ["travail", "coll√®gue", "√©tudes", "social", "pro", "vitalit√©", "isolement"]):
             profile["pro_info"] = user_message
             logger.info(f"Profile updated: pro_info set for {profile['name']}")


        # 2. Construction de l'instruction pour l'Agent (le System Prompt g√®re le ton, cette instruction g√®re la direction)
        
        history.append({"role": "user", "content": user_message, "ts": datetime.utcnow().isoformat()})
        
        agent_instruction = f"""
        L'utilisateur ({profile['name']}) a dit : "{user_message}".
        
        [CONTEXTE_DIAGNOSTIC]:
        Socle familial : {profile.get('socle_info', 'Manquant')}
        Lien social/Pro : {profile.get('pro_info', 'Manquant')}
        Ancrage G√©o : {profile.get('geo_info', 'Manquant')}
        
        Ton objectif est d'appliquer le Protocole PEC (Validation + Recadrage + Relance).
        
        R√®gle d'Agent :
        Si l'une des informations [CONTEXTE_DIAGNOSTIC] est encore 'Manquant', tente d'y revenir de mani√®re douce et naturelle, en l'int√©grant dans ta r√©ponse de Protocole PEC. N'utilise JAMAIS les termes "diagnostic" ou "question".
        """
        
        # On passe l'instruction √† l'IA pour qu'elle d√©cide.
        payload_messages = [{"role": "user", "content": agent_instruction}] + history

        # Call model
        response = await chat_with_ai(profile, payload_messages, context)

        # Stockage et r√©ponse
        history.append({"role": "assistant", "content": response, "ts": datetime.utcnow().isoformat()})
        context.user_data["history"] = history

        await update.message.reply_text(response)
        return

async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE):
    logger.exception("Exception: %s", context.error)

# -----------------------
# MAIN
# -----------------------
def main():
    if not TELEGRAM_BOT_TOKEN or not TOGETHER_API_KEY:
        logger.critical("Missing TELEGRAM_BOT_TOKEN or TOGETHER_API_KEY in environment.")
        return

    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    application.add_error_handler(error_handler)

    logger.info("Soph_IA V65 starting...")
    application.run_polling()

if __name__ == "__main__":
    main()