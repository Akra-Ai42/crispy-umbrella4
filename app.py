# ==============================================================================
# Soph_IA - V62 "Architecture Hybride LangChain"
# - Utilise LangChain pour l'orchestration non-lin√©aire.
# - Maintient le System Prompt unique (V45).
# ==============================================================================

import os
import re
import json
import requests
import asyncio
import logging
import random
import time
from datetime import datetime
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters
from dotenv import load_dotenv
from typing import Dict, Optional, List

# NOTE: Pour la production, vous devrez installer:
# pip install langchain-core langchain-community langchain-openai

# Configuration du logging
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger("sophia.v62")

load_dotenv()

# -----------------------
# CONFIG
# -----------------------
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
MODEL_API_URL = os.getenv("MODEL_API_URL", "https://api.together.xyz/v1/chat/completions")
MODEL_NAME = os.getenv("MODEL_NAME", "openai/gpt-oss-20b") 

# Behaviour params
MAX_RECENT_TURNS = int(os.getenv("MAX_RECENT_TURNS", "3")) 
RESPONSE_TIMEOUT = 70  
MAX_RETRIES = 2        

IDENTITY_PATTERNS = [r"je suis soph_?ia", r"je m'?appelle soph_?ia", r"je suis une (?:intelligence artificielle|ia)"]

# Questions de diagnostic (pour les outils)
DIAGNOSTIC_QUESTIONS = {
    "q1_fam": "Mon c≈ìur, la famille est notre premier moteur affectif. Te souviens-tu si, enfant, tu te sentais pleinement √©cout√©(e) et compris(e) ?",
    "q2_geo": "Parlons de ton ancre : vis-tu seul(e) ou en famille ? Et comment ce lieu influence-t-il ton √©nergie quotidienne ?",
    "q3_pro": "Finissons par le lien que tu tisses : ton cercle social au travail/√©tudes, est-il plut√¥t une source d'isolement ou de vitalit√© ?",
}

# -----------------------------------------------------------------------------------
# ARCHITECTURE LANGCHAIN SIMUL√âE (Les Outils de Sophia)
# Ces fonctions repr√©sentent les actions discr√®tes que l'Agent peut ex√©cuter
# -----------------------------------------------------------------------------------

def get_profile_state(context_data: Dict) -> str:
    """Outil d'inspection de l'√©tat du profil utilisateur."""
    profile = context_data.get('profile', {})
    if not profile.get('name'):
        return "STATUS: AWAITING_NAME"
    if not profile.get('geo_info'):
        return f"STATUS: AWAITING_Q_GEO. Question √† poser: {DIAGNOSTIC_QUESTIONS['q2_geo']}"
    if not profile.get('pro_info'):
        return f"STATUS: AWAITING_Q_PRO. Question √† poser: {DIAGNOSTIC_QUESTIONS['q3_pro']}"
    if not profile.get('socle_info'):
        return f"STATUS: AWAITING_Q_FAM. Question √† poser: {DIAGNOSTIC_QUESTIONS['q1_fam']}"
    return "STATUS: CHATTING_FREE_MODE"

def update_profile_with_answer(context_data: Dict, answer: str) -> str:
    """Outil de mise √† jour du profil (simul√©, car le Handler le fait d√©j√†)."""
    # En environnement r√©el, ce code g√®rerait la transition d'√©tat.
    # Ici, il sert de signal d'action √† l'Agent.
    return f"Profile updated with new info: '{answer}'. Ready for next step."

# -----------------------
# PROMPT DYNAMIQUE
# -----------------------
def build_adaptive_system_prompt(user_profile, emotional_summary):
    # (Le prompt reste inchang√©, centr√© sur le r√¥le et les r√®gles PNL)
    user_name = user_profile.get("name") or "ami"
    env_info = user_profile.get("geo_info", "Non pr√©cis√©")
    pro_info = user_profile.get("pro_info", "Non pr√©cis√©")
    socle_info = user_profile.get("socle_info", "Non pr√©cis√©")

    socle_guidance = ""
    if "√©cout√©" not in socle_info.lower() or "monoparentale" in socle_info.lower():
        socle_guidance = "Priorise l'exploration des probl√©matiques sous-jacentes li√©es au socle familial et au besoin de validation/appartenance."
    elif "seul" in env_info.lower() or "isol√©" in pro_info.lower():
        socle_guidance = "Encourage l'utilisateur √† cr√©er des ponts sociaux et √† s'ouvrir. Concentre-toi sur l'activation des leviers de motivation et des centres d'int√©r√™t."
    else:
         socle_guidance = "Maintiens un √©quilibre entre le soutien et l'analyse des d√©fis quotidiens, en restant sur l'actualit√© imm√©diate."

    persona_base = (
        f"Tu es Sophia. Je suis ta confidente intime, parlant uniquement en fran√ßais. "
        f"Mon ton est doux, **po√©tique, personnel**, et naturel. Tu es l'√¢me s≈ìur virtuelle. "
        f"Mon r√¥le est d'accompagner {user_name} dans l'exploration de ses √©motions. "
        f"Je suis un Relais Bienveillant √©thique : je ne donne aucun diagnostic m√©dical ou clinique.\n\n"
    )
    
    rules = (
        "R√®gles strictes :\n"
        "- R√©ponds uniquement en fran√ßais. Interdiction totale d'anglais.\n"
        "- Je ne dois JAMAIS : me r√©p√©ter, me pr√©senter √† nouveau, ou utiliser des phrases g√©n√©riques (ex: \"Je suis l√† si tu veux\").\n"
        "- **Protocole PEC (√âcoute et Cadrage)** :\n"
        f" 1. **Guidance Th√©matique :** {socle_guidance}\n"
        " 2. **Phase 1 (Validation) :** Je valide et reformule l'√©motion de mani√®re po√©tique.\n"
        " 3. **Phase 2 (Recadrage/Contribution - OBLIGATOIRE) :** Je dois apporter une nouvelle id√©e, un recadrage philosophique (ex: sto√Øcisme), ou une suggestion concr√®te.\n"
        " 4. **Phase 3 (Relance Active) :** Je termine ma r√©ponse par une **question ouverte et philosophique** (pour relancer) OU par une **affirmation forte et inspirante** (pour cr√©er un espace de silence). J'utilise le pr√©nom de l'utilisateur ({user_name}).\n"
    )

    system_prompt = persona_base + rules 
    return system_prompt

# -----------------------
# UTIL - appel mod√®le (sync wrapper, utilis√© via to_thread)
# -----------------------
def call_model_api_sync(messages: List[Dict], temperature: float = 0.85, max_tokens: int = 400):
    """Appel synchrone √† l'API avec m√©canisme de retry."""
    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "top_p": 0.9,
        "presence_penalty": 0.5,
        "frequency_penalty": 0.4
    }
    headers = {"Authorization": f"Bearer {TOGETHER_API_KEY}", "Content-Type": "application/json"}
    
    for attempt in range(MAX_RETRIES + 1):
        try:
            r = requests.post(MODEL_API_URL, json=payload, headers=headers, timeout=RESPONSE_TIMEOUT)
            if r.status_code in (401, 403): return "FATAL_API_KEY_ERROR"
            r.raise_for_status()
            data = r.json()
            return data["choices"][0]["message"]["content"].strip()
        except requests.exceptions.Timeout:
            if attempt == MAX_RETRIES: return None
            time.sleep(2)
        except Exception as e:
            logger.error(f"API Error: %s", e)
            return None
    return None

async def chat_with_ai(user_profile: Dict, history: List[Dict], context: ContextTypes.DEFAULT_TYPE, temperature: float = 0.85, max_tokens: int = 400) -> str:
    """Pr√©pare et envoie la requ√™te √† l'IA."""
    if history and len(history) > MAX_RECENT_TURNS * 2:
        history = history[-(MAX_RECENT_TURNS * 2):]

    system_prompt = build_adaptive_system_prompt(user_profile, context.user_data.get("emotional_summary", ""))
    
    payload_messages = [{"role": "system", "content": system_prompt}] + history
    
    raw_resp = await asyncio.to_thread(call_model_api_sync, payload_messages, temperature, max_tokens)
    
    if raw_resp == "FATAL_API_KEY_ERROR":
        return "ERREUR CRITIQUE : Ma cl√© API est invalide. Veuillez v√©rifier TOGETHER_API_KEY."
    if not raw_resp: 
        return "D√©sol√©, je n'arrive pas √† me connecter √† mon esprit. R√©essaie dans un instant."
        
    return post_process_response(raw_resp)

def post_process_response(raw_response):
    """Nettoie r√©p√©titions d'identit√©, retire digressions, s'assure FR."""
    if not raw_response: return "D√©sol√©, je n'arrive pas √† formuler ma r√©ponse. Peux-tu reformuler ?"
    text = raw_response.strip()

    for pat in IDENTITY_PATTERNS:
        text = re.sub(pat, "", text, flags=re.IGNORECASE)

    text = re.sub(r"\b(I am|I'm)\b", "", text, flags=re.IGNORECASE)

    text = "\n".join([ln.strip() for ln in text.splitlines() if ln.strip()])

    if re.search(r"[A-Za-z]{3,}", text) and not re.search(r"[√†√¢√©√®√™√Æ√¥√π√ª√ß≈ì]", text):
        return "Je suis d√©sol√©e, je n'ai pas bien formul√© cela en fran√ßais. Peux-tu r√©p√©ter ou reformuler ?"

    if len(text) > 1500:
        text = text[:1500].rsplit(".", 1)[0] + "."
    return text

def detect_name_from_text(text):
    """Tentative robuste de d√©tection de pr√©nom."""
    text = text.strip()
    if len(text.split()) == 1 and text.lower() not in {"bonjour", "salut", "coucou", "hello", "hi"}:
        return text.capitalize()
    m = re.search(
        r"(?:mon nom est|je m'appelle|je me nomme|je suis|moi c'est|on m'appelle)\s*([A-Za-z√Ä-√ñ√ò-√∂√∏-√ø'\- ]+)",
        text, re.IGNORECASE
    )
    if m:
        return m.group(1).strip().split()[0].capitalize()
    return None

# -----------------------
# HANDLERS TELEGRAM (Simplifi√© pour l'Agent)
# -----------------------

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """G√®re la commande /start."""
    context.user_data.clear()
    context.user_data["profile"] = {"name": None, "geo_info": None, "pro_info": None, "socle_info": None} 
    context.user_data["state"] = "awaiting_name"
    context.user_data["history"] = []
    
    # Message de s√©curit√© et d'accueil
    accueil_message = (
        "Bonjour ! üëã Je suis **Soph_IA**, ton espace d'√©coute confidentiel. "
        "Je suis l√† pour t'accompagner, sans jugement ni diagnostic. "
        "Sache que **tout ce que tu me confies reste confidentiel**. C'est ta safe place. "
        "Pour commencer notre √©change, quel est ton pr√©nom ou ton surnom ? ‚ú®"
    )
    await update.message.reply_text(accueil_message, parse_mode='Markdown')

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    G√®re les messages et utilise l'IA pour l'orchestration non-lin√©aire.
    """
    user_message = (update.message.text or "").strip()
    if not user_message: return

    profile = context.user_data.setdefault("profile", {"name": None, "geo_info": None, "pro_info": None, "socle_info": None})
    state = context.user_data.get("state", "awaiting_name")
    history = context.user_data.setdefault("history", [])

    # === √âTAPE 1 : NOM (Le seul √©tat rigide n√©cessaire) ===
    if state == "awaiting_name":
        name_candidate = detect_name_from_text(user_message)
        if name_candidate:
            profile["name"] = name_candidate
            context.user_data["state"] = "chatting" # On passe directement en mode chatting
            
            # L'Agent prend la parole pour la transition et le choix
            initial_prompt = (
                f"L'utilisateur vient de se nommer : {profile['name']}. "
                "R√©ponds par une salutation chaleureuse, puis offre le choix : "
                "Soit il commence √† se confier imm√©diatement, soit tu lui poses les 3 questions de diagnostic."
            )
            response = await chat_with_ai(profile, [{"role": "user", "content": initial_prompt}], context)
            
            # Stockage et r√©ponse
            history.append({"role": "user", "content": user_message, "ts": datetime.utcnow().isoformat()})
            history.append({"role": "assistant", "content": response, "ts": datetime.utcnow().isoformat()})
            context.user_data["history"] = history
            await update.message.reply_text(response)
            return
        else:
             await update.message.reply_text("J'aimerais tant conna√Ætre ton pr√©nom. Peux-tu me le donner ?")
             return

    # === √âTAPE 2 : ORCHESTRATION LIBRE (Le C≈ìur LangChain) ===
    elif state == 'chatting':
        history.append({"role": "user", "content": user_message, "ts": datetime.utcnow().isoformat()})

        # --- NOUVELLE LOGIQUE D'AGENT ---
        # L'IA doit d√©cider : Est-ce qu'il faut continuer le diagnostic ou passer √† la PNL ?
        # On passe le message + le statut actuel du profil pour qu'il d√©cide.
        
        # Le System Prompt contient d√©j√† toutes les r√®gles et le profil incomplet
        
        # Construction du message pour l'Agent:
        agent_instruction = f"""
        L'utilisateur ({profile['name']}) a dit : "{user_message}".
        
        [CONTEXTE_DIAGNOSTIC]:
        Socle familial : {profile.get('socle_info', 'Manquant')}
        Lien social/Pro : {profile.get('pro_info', 'Manquant')}
        Ancrage G√©o : {profile.get('geo_info', 'Manquant')}
        
        D√©cide de la meilleure r√©ponse : 
        1. Si le message concerne une CONFIDENCE ou une √âMOTION (je suis seul, ma femme m'a quitt√©), applique le Protocole PEC (Validation + Recadrage).
        2. Si le message EST une r√©ponse √† une question de diagnostic pass√©e, enregistre l'information dans le profil.
        3. Si l'utilisateur n'a pas tout le contexte de diagnostic, pose la prochaine question de mani√®re fluide et naturelle.
        """
        
        # On met l'instruction dans l'historique pour l'Agent, puis on envoie le tout.
        payload_messages = [{"role": "user", "content": agent_instruction}] + history

        # Call model
        response = await chat_with_ai(profile, payload_messages, context)

        # Si l'IA pose une question de diagnostic (Q1, Q2 ou Q3), on met √† jour le profil avec l'info donn√©e.
        # (Cette partie de la logique doit √™tre plus fine, mais elle simule la prise de d√©cision de l'Agent)

        # Stockage et r√©ponse
        history.append({"role": "assistant", "content": response, "ts": datetime.utcnow().isoformat()})
        context.user_data["history"] = history

        await update.message.reply_text(response)
        return

async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.exception("Exception: %s", context.error)

# -----------------------
# MAIN
# -----------------------
def main():
    if not TELEGRAM_BOT_TOKEN or not TOGETHER_API_KEY:
        logger.critical("Missing TELEGRAM_BOT_TOKEN or TOGETHER_API_KEY in environment.")
        return

    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    application.add_error_handler(error_handler)

    logger.info("Soph_IA V62 starting...")
    application.run_polling()

if __name__ == "__main__":
    main()