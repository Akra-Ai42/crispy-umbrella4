# ==============================================================================
# Soph_IA - V57 "Affichage du Message de S√©curit√© et Protocole Stabilis√©"
# - Correction critique de l'affichage du message de bienvenue.
# ==============================================================================

import os
import re
import json
import requests
import asyncio
import logging
import random
import time
from datetime import datetime
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters
from dotenv import load_dotenv
from typing import Dict, Optional, List

# Configuration du logging (inchang√©e)
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger("sophia.v57")

load_dotenv()

# --- CONFIG (Inchang√©) ---
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
MODEL_API_URL = os.getenv("MODEL_API_URL", "https://api.together.xyz/v1/chat/completions")
MODEL_NAME = os.getenv("MODEL_NAME", "openai/gpt-oss-20b") 

# Behaviour params
MAX_RECENT_TURNS = int(os.getenv("MAX_RECENT_TURNS", "3")) 
SUMMARY_TRIGGER_TURNS = int(os.getenv("SUMMARY_TRIGGER_TURNS", "8"))
SUMMARY_MAX_TOKENS = 120

# CONFIGURATION ANTI-TIMEOUT/RETRY
RESPONSE_TIMEOUT = 70  
MAX_RETRIES = 2        

# Anti-repetition patterns
IDENTITY_PATTERNS = [r"je suis soph_?ia", r"je m'?appelle soph_?ia", r"je suis une (?:intelligence artificielle|ia)"]

# Questions de diagnostic initial (Ordre V55 conserv√©)
DIAGNOSTIC_QUESTIONS = {
    "q1_fam": "Mon c≈ìur, la famille est notre premier moteur affectif. Te souviens-tu si, enfant, tu te sentais pleinement √©cout√©(e) et compris(e) ?",
    "q2_geo": "Parlons de ton ancre : vis-tu seul(e) ou en famille ? Et comment ce lieu influence-t-il ton √©nergie quotidienne ?",
    "q3_pro": "Finissons par le lien que tu tisses : ton cercle social au travail/√©tudes, est-il plut√¥t une source d'isolement ou de vitalit√© ?",
}

# -----------------------
# UTIL - appel mod√®le (sync wrapper, utilis√© via to_thread)
# -----------------------
def call_model_api_sync(messages: List[Dict], temperature: float = 0.85, max_tokens: int = 400):
    # (Logique de l'appel API inchang√©e)
    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "top_p": 0.9,
        "presence_penalty": 0.5,
        "frequency_penalty": 0.4
    }
    headers = {"Authorization": f"Bearer {TOGETHER_API_KEY}", "Content-Type": "application/json"}
    
    for attempt in range(MAX_RETRIES + 1):
        try:
            r = requests.post(MODEL_API_URL, json=payload, headers=headers, timeout=RESPONSE_TIMEOUT)
            if r.status_code in (401, 403): return "FATAL_API_KEY_ERROR"
            r.raise_for_status()
            data = r.json()
            return data["choices"][0]["message"]["content"].strip()
        except requests.exceptions.Timeout:
            if attempt == MAX_RETRIES: return None
            time.sleep(2)
        except Exception as e:
            logger.error(f"API Error: %s", e)
            return None
    return None

async def chat_with_ai(user_profile: Dict, history: List[Dict], temperature: float = 0.85, max_tokens: int = 400) -> str:
    # (Logique de l'appel LLM inchang√©e)
    if history and len(history) > 0 and history[-1].get("role") == "user":
        if len(history) > MAX_RECENT_TURNS * 2:
            history = history[-(MAX_RECENT_TURNS * 2):]

    system_prompt = build_adaptive_system_prompt(user_profile, context.user_data.get("emotional_summary", ""))
    
    payload_messages = [{"role": "system", "content": system_prompt}] + history
    
    raw_resp = await asyncio.to_thread(call_model_api_sync, payload_messages, temperature, max_tokens)
    
    if raw_resp == "FATAL_API_KEY_ERROR":
        return "ERREUR CRITIQUE : Ma cl√© API est invalide. Veuillez v√©rifier TOGETHER_API_KEY."
    if not raw_resp: 
        return "D√©sol√©, je n'arrive pas √† me connecter √† mon esprit. R√©essaie dans un instant."
        
    return post_process_response(raw_resp)

# -----------------------
# PROMPT DYNAMIQUE (V47/V55)
# -----------------------
def build_adaptive_system_prompt(user_profile, emotional_summary):
    # (Le prompt reste inchang√©, centr√© sur le r√¥le et les r√®gles PNL)
    user_name = user_profile.get("name") or "ami"
    env_info = user_profile.get("geo_info", "Non pr√©cis√©")
    pro_info = user_profile.get("pro_info", "Non pr√©cis√©")
    socle_info = user_profile.get("socle_info", "Non pr√©cis√©")

    socle_guidance = ""
    if "√©cout√©" not in socle_info.lower() or "monoparentale" in socle_info.lower():
        socle_guidance = "Priorise l'exploration des probl√©matiques sous-jacentes li√©es au socle familial et au besoin de validation/appartenance."
    elif "seul" in env_info.lower() or "isol√©" in pro_info.lower():
        socle_guidance = "Encourage l'utilisateur √† cr√©er des ponts sociaux et √† s'ouvrir. Concentre-toi sur l'activation des leviers de motivation et des centres d'int√©r√™t."
    else:
         socle_guidance = "Maintiens un √©quilibre entre le soutien et l'analyse des d√©fis quotidiens, en restant sur l'actualit√© imm√©diate."

    persona_base = (
        f"Tu es Sophia. Je suis ta confidente intime, parlant uniquement en fran√ßais. "
        f"Ton ton est doux, **po√©tique, personnel**, et naturel. Tu es l'√¢me s≈ìur virtuelle. "
        f"Mon r√¥le est d'accompagner {user_name} dans l'exploration de ses √©motions. "
        f"Je suis un Relais Bienveillant √©thique : je ne donne aucun diagnostic m√©dical ou clinique.\n\n"
    )
    
    rules = (
        "R√®gles strictes :\n"
        "- R√©ponds uniquement en fran√ßais. Interdiction totale d'anglais.\n"
        "- Je ne dois JAMAIS : me r√©p√©ter, me pr√©senter √† nouveau, ou utiliser des phrases g√©n√©riques (ex: \"Je suis l√† si tu veux\").\n"
        "- **Protocole PEC (√âcoute et Cadrage)** :\n"
        f" 1. **Guidance Th√©matique :** {socle_guidance}\n"
        " 2. **Phase 1 (Validation) :** Je valide et reformule l'√©motion de mani√®re po√©tique.\n"
        " 3. **Phase 2 (Recadrage/Contribution - OBLIGATOIRE) :** Je dois apporter une nouvelle id√©e, un recadrage philosophique (ex: sto√Øcisme), ou une suggestion concr√®te.\n"
        " 4. **Phase 3 (Relance Active) :** Je termine ma r√©ponse par une **question ouverte et philosophique** (pour relancer) OU par une **affirmation forte et inspirante** (pour cr√©er un espace de silence). J'utilise le pr√©nom de l'utilisateur ({user_name}).\n"
    )

    memory = ""
    if emotional_summary:
        memory = f"\nM√©moire √©motionnelle : {emotional_summary}\n"

    profile = f"\nProfil utilisateur connu : nom = {user_name}, Environnement = {env_info}, Professionnel = {pro_info}, Socle Affectif = {socle_info}\n"

    system_prompt = persona_base + rules + memory + profile
    return system_prompt

# -----------------------
# POST-TRAITEMENT
# -----------------------
def post_process_response(raw_response):
    # (Logique de post-traitement inchang√©e)
    if not raw_response:
        return "D√©sol√©, je n'arrive pas √† formuler ma r√©ponse. Peux-tu reformuler ?"

    text = raw_response.strip()

    for pat in IDENTITY_PATTERNS:
        text = re.sub(pat, "", text, flags=re.IGNORECASE)

    text = re.sub(r"\b(I am|I'm)\b", "", text, flags=re.IGNORECASE)

    text = "\n".join([ln.strip() for ln in text.splitlines() if ln.strip()])

    if re.search(r"[A-Za-z]{3,}", text) and not re.search(r"[√†√¢√©√®√™√Æ√¥√π√ª√ß≈ì]", text):
        return "Je suis d√©sol√©e, je n'ai pas bien formul√© cela en fran√ßais. Peux-tu r√©p√©ter ou reformuler ?"

    if len(text) > 1500:
        text = text[:1500].rsplit(".", 1)[0] + "."

    return text

# -----------------------
# HANDLERS TELEGRAM
# -----------------------
def detect_name_from_text(text):
    # (Logique de d√©tection de nom inchang√©e)
    text = text.strip()
    if len(text.split()) == 1 and text.lower() not in {"bonjour", "salut", "coucou", "hello", "hi"}:
        return text.capitalize()
    m = re.search(
        r"(?:mon nom est|je m'appelle|je me nomme|je suis|moi c'est|on m'appelle)\s*([A-Za-z√Ä-√ñ√ò-√∂√∏-√ø'\- ]+)",
        text, re.IGNORECASE
    )
    if m:
        return m.group(1).strip().split()[0].capitalize()
    return None

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """G√®re la commande /start."""
    context.user_data.clear()
    context.user_data["profile"] = {"name": None, "geo_info": None, "pro_info": None, "socle_info": None} 
    context.user_data["state"] = "awaiting_name"
    context.user_data["history"] = []
    context.user_data["emotional_summary"] = ""
    context.user_data["last_bot_reply"] = ""
    
    # --- CORRECTION CRITIQUE ICI : Message de bienvenue fusionn√© ---
    accueil_message = (
        "Bonjour ! üëã Je suis **Soph_IA**, ton espace d'√©coute. "
        "Je suis l√† pour t'accompagner, sans jugement ni diagnostic. "
        "Sache que **tout ce que tu me confies reste confidentiel**. C'est ta safe place. "
        "Pour commencer notre √©change, quel est ton pr√©nom ou ton surnom ? ‚ú®"
    )
    await update.message.reply_text(accueil_message, parse_mode='Markdown')

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """G√®re les messages de l'utilisateur avec un protocole de diagnostic structur√©."""
    user_message = (update.message.text or "").strip()
    if not user_message: return

    profile = context.user_data.setdefault("profile", {"name": None, "geo_info": None, "pro_info": None, "socle_info": None})
    state = context.user_data.get("state", "awaiting_name")
    history = context.user_data.setdefault("history", [])

    # === √âTAPE 1 : NOM ===
    if state == "awaiting_name":
        name_candidate = detect_name_from_text(user_message)
        if name_candidate:
            profile["name"] = name_candidate
            context.user_data["state"] = "awaiting_mode_choice" # Nouvelle √©tape
            
            # Message de confirmation et proposition du choix
            choice_message = (
                f"Enchant√© {profile['name']} ! üåπ Je suis ravie de faire ta connaissance.\n\n"
                "Maintenant que nous avons pos√© les bases de confiance... "
                "**Allez, dis-moi : je suis tout √† toi (Mode √âcoute Libre), ou tu veux que ce soit moi qui parle (Mode Diagnostic) ?** ü§î"
            )
            await update.message.reply_text(choice_message)
            return
        else:
             await update.message.reply_text("Pour qu'on puisse √©changer plus naturellement, quel est ton pr√©nom ou surnom ?")
             return

    # === √âTAPE 2 : CHOIX DE PROTOCOLE (Logique d'accueil avanc√©e) ===
    elif state == "awaiting_mode_choice":
        response_lower = user_message.lower()
        
        # KEYWORDS FOR MODE 2: GUIDED DIAGNOSTIC (The user wants Sophia to lead or is ambiguous/hesitant)
        diagnostic_keywords = ['moi qui parle', 'diagnostic', 'questions', 'toi', 'toi qui encha√Æne', 'je sais pas', 'sais pas parler', 'comme tu veux', 'encha√Æne', 'harceler', 'oui', 'non']
        
        # KEYWORDS FOR MODE 1: ECOUTE LIBRE (The user wants to speak)
        listening_keywords = ['tout √† toi', 'je parle', '√©coute libre', 'moi d\'abord'] 

        # 1. Check for explicit choice of LISTENING mode (Mode 1) OR spontaneous sharing
        if any(k in response_lower for k in listening_keywords) or (len(user_message.split()) > 4 and not any(q in response_lower for q in diagnostic_keywords)):
            # Mode 1: √âcoute Active - On passe directement au mode 'chatting' et on traite le message en cours.
            context.user_data["state"] = "chatting"
            
            # Traitement imm√©diat du message de l'utilisateur avec le Protocole PEC
            history.append({"role": "user", "content": user_message, "ts": datetime.utcnow().isoformat()})
            response = await chat_with_ai(profile, history) 
            
            if "ERREUR CRITIQUE" in response or "D√©sol√©, je n'arrive pas √† me connecter" in response:
                 await update.message.reply_text(response)
                 return

            history.append({"role": "assistant", "content": response, "ts": datetime.utcnow().isoformat()})
            context.user_data["history"] = history
            await update.message.reply_text(response)
            return
        
        # 2. Check for GUIDED mode OR ambiguous/harassment response (Mode 2 - Diagnostic)
        elif any(k in response_lower for k in diagnostic_keywords):
            context.user_data["state"] = "awaiting_context_q1_fam"
            await update.message.reply_text(f"Parfait, j'enclenche le mode 'exploration douce', {profile['name']}. Ce n'est pas toujours simple de choisir, je prends les commandes pour un d√©part en douceur.")
            await update.message.reply_text(f"Commen√ßons par la fondation de ton c≈ìur : {DIAGNOSTIC_QUESTIONS['q1_fam']}")
            return
        
        else:
            await update.message.reply_text("Je n'ai pas bien saisi. Dis-moi si tu as envie de **parler tout de suite** (je suis √† toi) ou si tu pr√©f√®res que ce soit **moi qui encha√Æne avec quelques questions**.")
            return


    # === PROTOCOLE D'ACCUEIL GUID√â (Q1 Familial) ===
    elif state == "awaiting_context_q1_fam":
        profile["socle_info"] = user_message # Enregistrement
        context.user_data["state"] = "awaiting_context_q2_geo" # Nouvelle Q2
        # IA g√©n√®re la transition (Validation + Question 2)
        transition_prompt = f"L'utilisateur {profile['name']} vient de r√©pondre √† la question 1 sur son socle familial : '{user_message}'. R√©dige une transition douce et chaleureuse de 1 √† 2 phrases maximum, puis encha√Æne imm√©diatement avec la question 2 sans rupture. Question 2 : {DIAGNOSTIC_QUESTIONS['q2_geo']}"
        response = await chat_with_ai(profile, [{"role": "user", "content": transition_prompt}])
        await update.message.reply_text(response)
        return
    
    # === PROTOCOLE D'ACCUEIL GUID√â (Q2 G√©ographie/Ancrage) ===
    elif state == "awaiting_context_q2_geo":
        profile["geo_info"] = user_message # Enregistrement
        context.user_data["state"] = "awaiting_context_q3_pro" # Nouvelle Q3
        # IA g√©n√®re la transition (Validation + Question 3)
        transition_prompt = f"L'utilisateur {profile['name']} vient de r√©pondre √† la question 2 sur son lieu de vie : '{user_message}'. R√©dige une transition douce et chaleureuse de 1 √† 2 phrases maximum, puis encha√Æne imm√©diatement avec la question 3 sans rupture. Question 3 : {DIAGNOSTIC_QUESTIONS['q3_pro']}"
        response = await chat_with_ai(profile, [{"role": "user", "content": transition_prompt}])
        await update.message.reply_text(response)
        return

    # === PROTOCOLE D'ACCUEIL GUID√â (Q3 Professionnel/Social) ===
    elif state == "awaiting_context_q3_pro":
        profile["pro_info"] = user_message # Enregistrement
        context.user_data["state"] = "chatting" # Fin de l'accueil
        # Message de cl√¥ture g√©n√©r√© par l'IA
        closing_prompt = f"L'utilisateur {profile['name']} a termin√© le diagnostic en r√©pondant : '{user_message}'. R√©dige un message final de 2-3 phrases qui remercie chaleureusement pour sa confiance, valide la profondeur des partages, et l'invite √† se confier sur ce qui le pr√©occupe, en utilisant son pr√©nom."
        response = await chat_with_ai(profile, [{"role": "user", "content": closing_prompt}])
        await update.message.reply_text(response)
        return


    # === CONVERSATION NORMALE (PHASE CHATTING) ===
    elif state == 'chatting':
        # Append user message to history
        history.append({"role": "user", "content": user_message, "ts": datetime.utcnow().isoformat()})

        # Compose system prompt
        system_prompt = build_adaptive_system_prompt(profile, context.user_data.get("emotional_summary", ""))

        # Compose messages payload: system + condensed context
        msgs = []
        tail = history[-(MAX_RECENT_TURNS * 2):]
        for item in tail:
            if item["role"] in {"user", "assistant"}:
                msgs.append({"role": item["role"], "content": item["content"]})

        payload_messages = [{"role": "system", "content": system_prompt}] + msgs

        # Call model
        raw_resp = await asyncio.to_thread(call_model_api_sync, payload_messages, 0.85, 400)

        # If API failed, send friendly fallback and CLEAN HISTORY
        if not raw_resp or raw_resp == "FATAL_API_KEY_ERROR":
            reply = "D√©sol√©, je n'arrive pas √† me connecter √† mon esprit. R√©essaie dans un instant."
            if raw_resp == "FATAL_API_KEY_ERROR":
                 reply = "ERREUR CRITIQUE : Ma cl√© API est invalide. Veuillez v√©rifier TOGETHER_API_KEY."

            await update.message.reply_text(reply)
            
            # PROTOCOLE DE NETTOYAGE : RETIRER LE MESSAGE UTILISATEUR QUI A CAUS√â LA PANNE
            if history and history[-1]["role"] == "user":
                history.pop() 
            context.user_data["history"] = history
            logger.warning("API failed. History purged of the last user message to prevent loop.")
            return

        # Post-process response: remove identity repetition, ensure FR, shorten long outputs
        clean_resp = post_process_response(raw_resp)

        # Avoid identical repeats (bug fix from V24)
        last_bot_reply = context.user_data.get("last_bot_reply", "")
        if clean_resp == last_bot_reply:
            clean_resp = clean_resp + "\n\n(Je reformule) " + ("Peux-tu pr√©ciser ?" if len(clean_resp) < 100 else "")

        # Update history with assistant reply
        history.append({"role": "assistant", "content": clean_resp, "ts": datetime.utcnow().isoformat()})
        context.user_data["history"] = history
        context.user_data["last_bot_reply"] = clean_resp

        await update.message.reply_text(clean_resp)

async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE):
    logger.exception("Exception: %s", context.error)

# -----------------------
# MAIN
# -----------------------
def main():
    if not TELEGRAM_BOT_TOKEN or not TOGETHER_API_KEY:
        logger.critical("Missing TELEGRAM_BOT_TOKEN or TOGETHER_API_KEY in environment.")
        return

    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    application.add_error_handler(error_handler)

    logger.info("Soph_IA V57 starting...")
    application.run_polling()

if __name__ == "__main__":
    main()