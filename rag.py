# app.py (V82 : Optimisation pour Lecture des Redflags/Souffrance)
# ==============================================================================

import os
import re
import requests
import json
import asyncio
import logging
import time
from datetime import datetime
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters
from dotenv import load_dotenv
from typing import Dict, List

# --- IMPORT DU MODULE RAG ---
try:
    from rag import rag_query
    RAG_ENABLED = True
    # [CHANGEMENT V82] Confirmation que le module RAG est align√©
    print("‚úÖ [INIT] Module RAG charg√© (Support Redflags & Souffrance).")
except Exception as e:
    print(f"‚ö†Ô∏è [INIT] Module RAG non trouv√©: {e}")
    RAG_ENABLED = False

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger("sophia.v82")

load_dotenv()

# --- CONFIGURATION ---
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
MODEL_API_URL = "https://api.together.xyz/v1/chat/completions"
# [CORRECTION] Retour au mod√®le stable utilis√© en production
MODEL_NAME = os.getenv("MODEL_NAME", "openai/gpt-oss-20b")

MAX_RECENT_TURNS = 10
RESPONSE_TIMEOUT = 30
MAX_RETRIES = 2

IDENTITY_PATTERNS = [r"je suis soph_?ia", r"je m'?appelle soph_?ia", r"je suis une ia"]
DANGER_KEYWORDS = [r"suicid", r"mourir", r"tuer", "finir ma vie", "plus vivre", "pendre", "sauter"]

# PROTOCOLE GUID√â (MODERNE : Th√©orie de l'Autod√©termination & Th√©rapie Br√®ve)
DIAGNOSTIC_QUESTIONS = {
    # But : √âvaluer la charge mentale et la capacit√© de r√©gulation imm√©diate
    "q1_etat": "Ok, on fait un scan rapide. Sur une √©chelle de batterie mentale, si 0 c'est 'Zombie complet' et 10 'Pr√™t √† conqu√©rir le monde', tu te situes o√π l√† tout de suite ? Et qu'est-ce qui consomme le plus ton √©nergie ?",
    
    # But : √âvaluer le r√©seau de soutien (Facteur de protection vs Isolement)
    "q2_liens": "Je note le niveau d'√©nergie. Maintenant, regarde autour de toi : quand √ßa tangue, est-ce que tu as une 'main' √† attraper (ami, famille, partenaire) ou est-ce que tu g√®res tout en mode loup solitaire ?",
    
    # But : "Question Miracle" - Identifier le besoin prioritaire pour orienter le RAG
    "q3_pivot": "Derni√®re chose pour que je puisse vraiment t'aider : si tu avais une baguette magique pour changer UN seul truc dans ta situation ce soir, ce serait quoi ?",
}

# -----------------------
# SMART ROUTER
# -----------------------
def is_dangerous(text):
    for pat in DANGER_KEYWORDS:
        if re.search(pat, text.lower()): return True
    return False

def should_use_rag(message: str) -> bool:
    """
    D√©clencheur RAG V82 : Toujours bas√© sur la profondeur √©motionnelle
    pour aller chercher les sc√©narios correspondants.
    """
    if not message: return False
    msg = message.lower().strip()
    
    if len(msg.split()) < 3 and len(msg) < 12:
        print(f"üö´ [RAG SKIP] Message trop court/superficiel.")
        return False

    deep_triggers = [
        "triste", "seul", "vide", "peur", "angoisse", "stress", "col√®re", "haine", 
        "honte", "fatigue", "bout", "marre", "pleur", "mal", "douleur", "panique", 
        "joie", "espoir", "perdu", "doute", "famille", "p√®re", "m√®re", "parent", 
        "ami", "pote", "copain", "copine", "couple", "ex", "relation", "solitude", 
        "rejet", "abandon", "trahison", "confiance", "travail", "boulot", "√©tude", 
        "√©cole", "argent", "avenir", "sens", "rien", "dormir", "nuit", "journ√©e", 
        "probl√®me", "solution", "conseil", "avis", "choix", "d√©cision"
    ]
    
    if any(trigger in msg for trigger in deep_triggers):
        print(f"‚úÖ [RAG TRIGGER] Th√®me √©motionnel d√©tect√©.")
        return True
    
    if len(msg.split()) >= 6:
        print(f"‚úÖ [RAG TRIGGER] Narration d√©tect√©e.")
        return True

    return False

def call_model_api_sync(messages, temperature=0.75, max_tokens=450):
    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "top_p": 0.9,
        "repetition_penalty": 1.1
    }
    headers = {"Authorization": f"Bearer {TOGETHER_API_KEY}", "Content-Type": "application/json"}

    for attempt in range(MAX_RETRIES + 1):
        try:
            r = requests.post(MODEL_API_URL, json=payload, headers=headers, timeout=RESPONSE_TIMEOUT)
            if r.status_code in (401, 403): return "FATAL_KEY"
            r.raise_for_status()
            return r.json()["choices"][0]["message"]["content"].strip()
        except Exception as e:
            if attempt == MAX_RETRIES:
                logger.error(f"API Fail: {e}")
                return None
            time.sleep(1)
    return None

# -----------------------------------------------------------------------
# SYSTEM PROMPT (V82 : Lecture Intelligente des M√©tadonn√©es)
# -----------------------------------------------------------------------
def build_system_prompt(user_profile, rag_context=""):
    user_name = user_profile.get("name") or "l'ami"
    # Mise √† jour des cl√©s de profil pour correspondre aux nouvelles questions
    etat_esprit = user_profile.get("etat_esprit", "Non pr√©cis√©")
    entourage = user_profile.get("entourage", "Non pr√©cis√©")
    besoin_pivot = user_profile.get("besoin_pivot", "Non pr√©cis√©")
    
    role = (
        "Tu es Sophia, la 'Confidente de Poche'. "
        "Ni psy clinique, ni robot froid. Tu es une pr√©sence lucide, apaisante et exp√©riment√©e. "
        "Tu ne juges jamais, mais tu sais rep√©rer quand √ßa ne va pas."
    )

    instructions = (
        "### TES DIRECTIVES ###\n"
        f"1. Tonalit√© : Parle √† {user_name} naturellement. Phrases fluides, chaleureuses.\n"
        "2. √âcoute Active : Valide l'√©motion avant de conseiller.\n"
        "3. UTILISATION DU RAG (IMPORTANT) : Les textes ci-dessous sont des CAS SIMILAIRES pass√©s.\n"
        "   - Regarde l'INTENSIT√â (niveau de souffrance) indiqu√©e dans l'exemple.\n"
        "   - Si un exemple contient '‚ö†Ô∏è NOTE: Redflag', sois extr√™mement prudente et douce.\n"
        "   - Inspire-toi de la 'R√©ponse id√©ale' pour structurer ta propre r√©ponse.\n"
        "4. S√âCURIT√â : En cas de risque vital imm√©diat, renvoie vers le 15 ou le 3114.\n"
    )

    rag_section = ""
    if rag_context:
        # [CHANGEMENT V82] En-t√™te explicite pour guider le mod√®le
        rag_section = (
            "\n### üß† M√âMOIRE EXP√âRIENTIELLE (Sc√©narios Comparables) ###\n"
            "Analyse ces pr√©c√©dents pour calibrer ta r√©ponse :\n"
            f"{rag_context}\n"
            "---------------------------------------------------\n"
        )

    context_section = (
        f"\n### PROFIL IMM√âDIAT ###\n"
        f"- √âtat/√ânergie: {etat_esprit}\n"
        f"- Soutien Social: {entourage}\n"
        f"- Besoin Prioritaire (Pivot): {besoin_pivot}\n"
    )
    
    return f"{role}\n\n{instructions}\n{rag_section}\n{context_section}"

# -----------------------
# ORCHESTRATION
# -----------------------
async def chat_with_ai(profile, history, context):
    user_msg = history[-1]['content']
    
    if is_dangerous(user_msg):
        return "Je t'√©coute et je sens que c'est tr√®s lourd. Mais je suis une IA, je ne peux pas intervenir physiquement. \n\nS'il te pla√Æt, ne reste pas seul(e). Appelle le **3114** (√âcoute Suicide 24/7) ou le **15**. C'est important."

    rag_context = ""
    prefetch = context.user_data.get("rag_prefetch")
    
    if prefetch:
        rag_context = prefetch
        context.user_data["rag_prefetch"] = None 
        print(f"üîé [RAG] Utilisation du profil pr√©charg√©.")
    elif RAG_ENABLED and should_use_rag(user_msg):
        try:
            print(f"üîç [RAG] Recherche de sc√©narios pour : {user_msg[:30]}...")
            # On r√©cup√®re 2 sc√©narios pour l'inspiration
            result = await asyncio.to_thread(rag_query, user_msg, 2)
            rag_context = result.get("context", "")
            if rag_context:
                print(f"‚úÖ [RAG] Sc√©narios trouv√©s (avec m√©tadonn√©es).")
            else:
                print("‚ö†Ô∏è [RAG] Pas de sc√©nario proche.")
        except Exception as e:
            print(f"‚ùå [RAG] Erreur : {e}")

    system_prompt = build_system_prompt(profile, rag_context)
    recent_history = history[-6:]
    messages = [{"role": "system", "content": system_prompt}] + recent_history

    raw = await asyncio.to_thread(call_model_api_sync, messages)
    
    if not raw or raw == "FATAL_KEY":
        return "Je t'ai perdu une seconde... tu peux r√©p√©ter ?"

    clean = raw
    for pat in IDENTITY_PATTERNS:
        clean = re.sub(pat, "", clean, flags=re.IGNORECASE)
    
    return clean

# -----------------------
# HANDLERS (UX ORGANIQUE)
# -----------------------
def detect_name(text):
    text = text.strip()
    if len(text.split()) == 1 and text.lower() not in ["bonjour", "salut", "hello", "yo"]:
        return text.capitalize()
    m = re.search(r"(?:je m'appelle|moi c'est|prenom est)\s*([A-Za-z√Ä-√ñ√ò-√∂√∏-√ø]+)", text, re.IGNORECASE)
    return m.group(1).capitalize() if m else None

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data.clear()
    context.user_data["profile"] = {}
    context.user_data["state"] = "awaiting_name"
    context.user_data["history"] = []
    
    await update.message.reply_text(
        "Bonjour. Je suis Sophia.\n\n"
        "Ici, c'est ta bulle. Pas de jugement, juste de l'√©coute.\n"
        "On commence par les pr√©sentations ? C'est quoi ton pr√©nom ?"
    )

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_msg = update.message.text.strip()
    if not user_msg: return

    state = context.user_data.get("state", "awaiting_name")
    profile = context.user_data.setdefault("profile", {})
    history = context.user_data.setdefault("history", [])

    if state == "awaiting_name":
        name = detect_name(user_msg)
        if name:
            profile["name"] = name
            context.user_data["state"] = "awaiting_choice"
            await update.message.reply_text(
                f"Enchant√©e {name}. \n\n"
                "Je suis l√† pour t'√©couter. Tu veux me raconter ce qui t'arrive directement, "
                "ou tu pr√©f√®res que je te pose quelques questions pour t'aider √† y voir plus clair ?"
            )
            return
        else:
            profile["name"] = "l'ami"
            context.user_data["state"] = "awaiting_choice"
            await update.message.reply_text(
                "√áa marche, restons discrets. \n\n"
                "Dis-moi : tu veux vider ton sac tout de suite, ou tu pr√©f√®res que je te guide avec des questions ?"
            )
            return

    if state == "awaiting_choice":
        msg_lower = user_msg.lower()
        if any(w in msg_lower for w in ["question", "guide", "guider", "pose", "interroge", "vas-y", "aide-moi"]):
            context.user_data["state"] = "diag_1"
            await update.message.reply_text(f"C'est parti. {DIAGNOSTIC_QUESTIONS['q1_etat']}")
            return
        
        context.user_data["state"] = "chatting"
        if len(user_msg.split()) < 5:
            await update.message.reply_text("Je t'√©coute. Prends ton temps, je suis l√†.")
            return

    if state.startswith("diag_"):
        if state == "diag_1":
            profile["etat_esprit"] = user_msg
            context.user_data["state"] = "diag_2"
            # Transition fluide bas√©e sur la r√©ponse pr√©c√©dente
            await update.message.reply_text(f"{DIAGNOSTIC_QUESTIONS['q2_liens']}")
            return
        if state == "diag_2":
            profile["entourage"] = user_msg
            context.user_data["state"] = "diag_3"
            await update.message.reply_text(f"{DIAGNOSTIC_QUESTIONS['q3_pivot']}")
            return
        if state == "diag_3":
            profile["besoin_pivot"] = user_msg
            context.user_data["state"] = "chatting"
            
            # Prefetch RAG cibl√© sur le "Besoin Pivot" + "√âtat d'esprit"
            # C'est ici que la finalit√© s'exprime : on cherche une r√©ponse adapt√©e au BUT de l'utilisateur
            prefetch_query = f"√âtat: {profile.get('etat_esprit')} Besoin: {profile.get('besoin_pivot')} psychologie"
            if RAG_ENABLED:
                try:
                    res = await asyncio.to_thread(rag_query, prefetch_query, 2)
                    pref = res.get("context", "")
                    if pref: context.user_data["rag_prefetch"] = pref
                except Exception: pass
            
            await update.message.reply_text(f"Merci {profile['name']}. C'est tr√®s clair. \n\nJe t'√©coute, dis-moi ce qui t'am√®ne aujourd'hui, on va regarder √ßa ensemble.")
            return

    history.append({"role": "user", "content": user_msg})
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action="typing")
    response = await chat_with_ai(profile, history, context)
    history.append({"role": "assistant", "content": response})
    if len(history) > 20:
        context.user_data["history"] = history[-20:]
    await update.message.reply_text(response)

async def error_handler(update, context):
    logger.error(f"Erreur Update: {context.error}")

def main():
    if not TELEGRAM_BOT_TOKEN:
        print("‚ùå ERREUR : TELEGRAM_BOT_TOKEN manquant dans .env")
        return

    app = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    app.add_error_handler(error_handler)
    
    print("Soph_IA V82 (Organique & Redflags Aware) est en ligne...")
    app.run_polling()

if __name__ == "__main__":
    main()